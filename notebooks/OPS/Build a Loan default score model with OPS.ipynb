{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Build a Loan default scoring model in OPS</b></th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a simple example to build and deploy a Machine Learning model ready to be used in Automation Decision Service.\n",
    "The deployed model is stored in  Open Prediction Service endpoint. You can find information about OPS implementations in this [documentation](https://github.com/IBM/open-prediction-service-hub).\n",
    "This other [notebook](https://github.com/icp4a/automation-decision-services-samples/tree/master/samples/MLNotebooks/Predict%20loan%20default%20with%20scikit-learn%20in%20WML.ipynb)\n",
    "builds the same model and stores it in Watson Machine Learning.\n",
    "\n",
    "Note that this model is built on a small synthetic dataset to serve as an example, its predictions are not realistic.\n",
    "\n",
    "After running this notebook, you can use the deployed model in a decision project in Automation Decision Service. You find a detailed description for this kind of integration in the [ML Start tutorial](https://github.com/icp4a/automation-decision-services-samples/tree/21.0.1/samples/MLStart).\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- You need to have an Open Prediction Service instance up and running.\n",
    "- To use the notebooks, follow the documentation [Creating a project](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/projects.html?audience=wdp).\n",
    "- Others notebooks are available in this [Samples documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-samples-overview.html).\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "-  Load a CSV file into a Pandas DataFrame.\n",
    "-  Explore data.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create a scikit-learn machine learning model.\n",
    "-  Store a machine learning model in the Open Prediction Service provider.\n",
    "-  Train and evaluate a model.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Load and explore data](#load)\n",
    "2.\t[Create a Scikit learn machine learning model](#model)\n",
    "3.\t[Store the model in Open Prediction Service provider](#provider)\n",
    "4.\t[Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 1. Load and explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will load the data as a Pandas DataFrame and perform a basic exploration.\n",
    "\n",
    "Load the data to the Pandas DataFrame by using *wget* to upload the data to gpfs and then use pandas *read* method to read data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wget if you don't already have it.\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall --yes scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'numpy>=1.19.5'\n",
    "!pip install 'pandas>=1.1.2'\n",
    "!pip install 'scikit-learn==0.23.2'\n",
    "!pip install 'requests>=2.25.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "link_to_data = 'https://raw.githubusercontent.com/ODMDev/decisions-on-spark/master/data/miniloan/miniloan-payment-default-cases-v2.0.csv'\n",
    "filename = wget.download(link_to_data)\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraires to create our Panda DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file to Pandas DataFrame using code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_names = ['creditScore', 'income', 'loanAmount', 'monthDuration', 'rate', 'yearlyReimbursement', 'paymentDefault']\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filename,\n",
    "    header=0,\n",
    "    delimiter=r'\\s*,\\s*',\n",
    "    engine='python'\n",
    ").replace(\n",
    "    [np.inf, -np.inf], np.nan\n",
    ").dropna().loc[:, used_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now explore the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all columns of DataFrame to float to avoid scaler warnings\n",
    "df = df.astype({'creditScore': float, \"income\": np.float64, \"loanAmount\": np.float64, \"monthDuration\": np.float64, \"yearlyReimbursement\": np.float64, \"paymentDefault\": np.float64})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data contains five fields. default field is the one you would like to predict (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records: \" + str(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 2. Create a Scikit learn machine learning model\n",
    "\n",
    "In this section you will learn how to:\n",
    "\n",
    "- [3.1 Prepare data](#prep)\n",
    "- [3.2 Create a model](#pipe)\n",
    "- [3.3 Train a model](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare data<a id=\"prep\"></a>\n",
    "\n",
    "In this subsection you will split your data into: \n",
    "- train data set\n",
    "- test data set\n",
    "- predict data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data = np.split(df.sample(frac=1, random_state=42), [int(.7*len(df)), int((.7+.2)*len(df))])\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "predict_data = splitted_data[2]\n",
    "\n",
    "print(\"Number of training records: \" + str(len(train_data)))\n",
    "print(\"Number of testing records : \" + str(len(test_data)))\n",
    "print(\"Number of prediction records : \" + str(len(predict_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see your data has been successfully split into three data sets: \n",
    "\n",
    "-  The train data set, which is the largest group, is used for training.\n",
    "-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n",
    "-  The predict data set will be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create a model<a id=\"pipe\"></a>\n",
    "\n",
    "In this section you will create a Scikit-Learn machine learning model and then train the model.\n",
    "\n",
    "In the first step you need to import the Scikit-Learn machine learning packages that will be needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the model. A linear model with Stochastic Gradient Descent is used in the following example. We use a pipeline to add an input scaling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=42, tol=1e-3)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('standardize', scaler),\n",
    "    (\"classifier\", clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the model<a id=\"train\"></a>\n",
    "Now, you can train your Random Forest model by using the previously defined **pipeline** and **train data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = train_data.loc[:, used_names[:-1]]\n",
    "y_train_data = train_data.loc[:, used_names[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train_data, y_train_data)\n",
    "\n",
    "# we defined a variable trainedAt to keep track of when the model was trained\n",
    "import datetime\n",
    "ts = datetime.datetime.now()\n",
    "trainedAt = ts.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your **model accuracy** now. Use **test data** to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_data = test_data.loc[:, used_names[:-1]]\n",
    "y_test_data = test_data.loc[:, used_names[-1]]\n",
    "\n",
    "predictions = pipeline.predict(x_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a **metrics** variable to keep track of the metrics values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, classification_report, balanced_accuracy_score, accuracy_score, confusion_matrix\n",
    "\n",
    "metrics = []\n",
    "\n",
    "name = \"Coefficient of determination R^2\"\n",
    "r2 = pipeline.score(x_test_data, y_test_data)\n",
    "metrics.append({ \"name\": name, \"value\": r2 })\n",
    "\n",
    "name = \"Root Mean Squared Error (RMSE)\"\n",
    "rmse = mean_squared_error(y_test_data, predictions)\n",
    "metrics.append({ \"name\": name, \"value\": rmse })\n",
    "\n",
    "name = \"Accuracy\"\n",
    "acc = accuracy_score(y_test_data, predictions)\n",
    "metrics.append({ \"name\": name, \"value\": acc })\n",
    "\n",
    "name = \"Balanced accuracy\"\n",
    "balanced_acc = balanced_accuracy_score(y_test_data, predictions)\n",
    "metrics.append({ \"name\": name, \"value\": balanced_acc })\n",
    "\n",
    "name = \"Confusion Matrix\"\n",
    "confusion_mat = confusion_matrix(y_test_data, predictions, labels=[0, 1])\n",
    "metrics.append({ \"name\": name, \"value\": str(confusion_mat.tolist()) })\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric[\"name\"], \"on test data =\", metric[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_data, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"provider\"></a>\n",
    "## 3. Store the model in Open Prediction Service provider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn how to use Python client libraries to store your model in your Open Prediction Service.\n",
    "\n",
    "- [3.1 Set up](#lib)\n",
    "- [3.2 Deploy model](#save)\n",
    "- [3.3 Invoke the model](#load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set up <a id=\"lib\"></a>\n",
    "\n",
    "In order to save your model into your Open Prediction Service\n",
    "You must first:\n",
    "\n",
    "- Check that your Open Prediction Service is up and running\n",
    "- Define a model configuration\n",
    "- Save your model in a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that your Open Prediction Service is up and running\n",
    "\n",
    "**Action**: Enter your Open Prediction Service URL instance in the cell above. Change its type to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPS_REQUEST_URL = 'PUT OPS URL '  # For local test: 'http://localhost:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin, urlparse\n",
    "import json, requests\n",
    "\n",
    "# Checking that that Open Prediction Service is up and running\n",
    "parsedUrl = urlparse(OPS_REQUEST_URL)\n",
    "statusUrl = urljoin(OPS_REQUEST_URL, urljoin(parsedUrl.path, 'info'))\n",
    "r = requests.get(statusUrl)\n",
    "\n",
    "status = r.status_code == requests.codes.ok\n",
    "versions= json.loads(r.text)[u'info']['libraries']\n",
    "if status:\n",
    "    print('Open Prediction Service is up and running.')\n",
    "    print('OPS scikit-learn version is : '+ versions['scikit-learn'])\n",
    "else:\n",
    "    print('An error occured when reaching out to your Open Prediction Service instance', r.status_code, r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a working output :\n",
    "Open Prediction Service is up and running.\n",
    "OPS sklearn version is : 0.23.2\n",
    "\n",
    "Next you need to define a configuration for your model\n",
    "\n",
    "**Action**: Complete all required data in the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters defining the model unicity:\n",
    "MODEL_NAME = \"loan-risk\"\n",
    "MODEL_VERSION = \"v0\"\n",
    "\n",
    "# Metadata\n",
    "METADATA_DESCRIPTION = \"Sample loan risk predictive model\"\n",
    "METADATA_AUTHOR = \"ADD_YOUR_NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are automating the input and output schema generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import build_table_schema\n",
    "\n",
    "mappingToOPSSchema = {\n",
    "    'integer': 'int64',\n",
    "    'number': 'float64'\n",
    "}\n",
    "\n",
    "def getInputSchema(dataFrame):\n",
    "    inputSchema = build_table_schema(dataFrame, index=False, version=False)\n",
    "\n",
    "    for index, field in enumerate(inputSchema['fields']):\n",
    "        inputSchema['fields'][index]['type'] = mappingToOPSSchema[field['type']]\n",
    "        inputSchema['fields'][index]['order'] = index\n",
    "    return inputSchema['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving input schema\n",
    "inputSchema = getInputSchema(x_train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have a complete configuration object to be bundled with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_configuration = {\n",
    "  \"name\": MODEL_NAME,\n",
    "  \"version\": MODEL_VERSION,\n",
    "  \"input_schema\": inputSchema,\n",
    "  \"metadata\": {\n",
    "    \"description\": METADATA_DESCRIPTION,\n",
    "    \"author\": METADATA_AUTHOR,\n",
    "    \"trained_at\": trainedAt,\n",
    "    \"metrics\": metrics\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(model_configuration, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Deploy model<a id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to add first the model configurartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUrl = urljoin(OPS_REQUEST_URL, urljoin(parsedUrl.path, 'models'))\n",
    "body= json.dumps(model_configuration)\n",
    "\n",
    "r=requests.post(modelUrl, data=body)\n",
    "status= r.status_code == requests.codes.ok\n",
    "content= json.loads(r.text)\n",
    "model_id= (content[u'id'])\n",
    "\n",
    "if status:\n",
    "    print('Model configuration was succesfully added.')\n",
    "else:\n",
    "    print('Model configuration was not added:', r.status_code, r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your model in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model_pickle(pickle_filename, model):\n",
    "        with open(pickle_filename, 'wb') as f:\n",
    "            pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_filename = MODEL_NAME + '-' + MODEL_VERSION + '-archive.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_pickle(pickle_filename, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUrl = urljoin(OPS_REQUEST_URL, urljoin(parsedUrl.path, 'models/'+ model_id ))\n",
    "print(modelUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {'file': open(pickle_filename, 'rb')}\n",
    "r = requests.post(modelUrl,data={'format':'pickle', 'input_data_structure': 'DataFrame', 'output_data_structure': 'ndarray'},files=files)\n",
    "if r.status_code == 201:\n",
    "    print(\"Model was succesfully added.\")\n",
    "else:\n",
    "    print('Model was not deployed:', r.status_code, r.text)\n",
    "    print('You might want to check if your model does not already exist under the same name and version.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Invoke the model<a id=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invokeUrl = urljoin(OPS_REQUEST_URL, urljoin(parsedUrl.path, '/predictions'))\n",
    "print(invokeUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict_data = predict_data.loc[:, used_names[:-1]]\n",
    "y_predict_data = predict_data.loc[:, used_names[-1]]\n",
    "\n",
    "raw_predict_data = x_predict_data.to_numpy()\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "data = {\n",
    "  \"target\": [\n",
    "    {\n",
    "      \"rel\": \"endpoint\",\n",
    "      \"href\": \"/endpoints/\"+ model_id\n",
    "    }\n",
    "  ],\n",
    "  \"parameters\": []\n",
    "}\n",
    "featureLabels = x_predict_data.columns\n",
    "\n",
    "predictions_np = []\n",
    "\n",
    "for row in raw_predict_data:\n",
    "    tmpData = deepcopy(data)\n",
    "    for index, value in enumerate(row):\n",
    "        tmpData['parameters'].append({\n",
    "            \"name\": featureLabels[index],\n",
    "            \"value\": value\n",
    "        })\n",
    "    print(json.dumps(tmpData, indent=4))\n",
    "    r = requests.post(invokeUrl,  data=json.dumps(tmpData))\n",
    "    status = r.status_code == requests.codes.ok\n",
    "    if status:\n",
    "        result = json.loads(r.text)[u'result'][u'predictions']\n",
    "        predictions_np.append(result)\n",
    "    else:\n",
    "        print('Model was not invoked:', r.status_code, r.text)\n",
    "        break\n",
    "\n",
    "predictions_np = np.array(predictions_np, dtype=int)\n",
    "\n",
    "predictions = pd.DataFrame(data=predictions_np, columns=[\"prediction\"]).astype({\"prediction\": bool})\n",
    "y_predict_data = y_predict_data.astype({\"paymentDefault\": bool})\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_predict_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_acc = balanced_accuracy_score(y_predict_data, predictions)\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_predict_data, predictions, labels=[0, 1])\n",
    "\n",
    "acc = accuracy_score(y_predict_data, predictions)\n",
    "\n",
    "print('Accuracy', acc)\n",
    "print('Balanced accuracy', balanced_acc)\n",
    "print('Confusion Matrix', confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 4. Summary and next steps\n",
    "You successfully completed this notebook! \n",
    " \n",
    "You learned how to use Scikit Learn machine learning API as well as Open Prediction Service for model creation and deployment. \n",
    " \n",
    "Now you can use this model deployment in a predictive model in Automation Decision Service. You find a detailed description for this kind of integration in the [ML Start tutorial](https://github.com/icp4a/automation-decision-services-samples/tree/21.0.1/samples/MLStart)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "This notebook was inspired by original notebook written by Pierre Feillet using Apache Spark and Watson Machine Learning.\n",
    "It was adapted for Scikit Learn and Open Prediction Service by Amel Ben Othmane. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "458px",
    "left": "10px",
    "top": "150px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}