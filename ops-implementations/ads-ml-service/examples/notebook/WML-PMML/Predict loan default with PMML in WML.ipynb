{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Build a Loan default PMML scoring model with scikit-learn in Watson ML </b></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains steps and code to get a loan dataset, create a predictive model, and start scoring new data. This notebook introduces commands for getting data and for basic data cleaning and exploration, model creation, model training, model persistence, model deployment, and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "-  Load a CSV file into a Pandas DataFrame.\n",
    "-  Explore data.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create a scikit-learn machine learning model.\n",
    "-  Train and evaluate a model.\n",
    "-  Save the model as PMML file.\n",
    "\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Set up](#setup)\n",
    "2.\t[Load and explore data](#load)\n",
    "3.\t[Create a Scikit learn machine learning model](#model)\n",
    "4.\t[Store the model in Watson Machine Learning provider](#provider)\n",
    "5.\t[Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Set up\n",
    "\n",
    "Before you use the sample code in this notebook,you create a <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a lite plan is offered and information about how to create the instance is <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 2. Load and explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will load the data as a Pandas DataFrame and perform a basic exploration.\n",
    "\n",
    "Load the data to the Pandas DataFrame by using *wget* to upload the data to gpfs and then use pandas *read* method to read data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wget if you don't already have it.\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "link_to_data = 'https://raw.githubusercontent.com/ODMDev/decisions-on-spark/master/data/miniloan/miniloan-payment-default-cases-v2.0.csv'\n",
    "filename = wget.download(link_to_data)\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraires to create our Panda DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file to Pandas DataFrame using code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_names = ['creditScore', 'income', 'loanAmount', 'monthDuration', 'rate', 'yearlyReimbursement', 'paymentDefault']\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filename,\n",
    "    header=0,\n",
    "    delimiter=r'\\s*,\\s*',\n",
    "    engine='python'\n",
    ").replace(\n",
    "    [np.inf, -np.inf], np.nan\n",
    ").dropna().loc[:, used_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the loaded data by using the following Pandas DataFrame methods:\n",
    "-  print types\n",
    "-  print top ten records\n",
    "-  count all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all columns of DataFrame to float to avoid scaler warnings\n",
    "df = df.astype({'creditScore': float, \"income\": np.float64, \"loanAmount\": np.float64, \"monthDuration\": np.float64, \"yearlyReimbursement\": np.float64, \"paymentDefault\": np.int64})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data contains five fields. default field is the one you would like to predict (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records: \" + str(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 3. Create a Scikit learn machine learning model\n",
    "\n",
    "In this section you will learn how to:\n",
    "\n",
    "- [3.1 Prepare data](#prep)\n",
    "- [3.2 Create a model](#pipe)\n",
    "- [3.3 Train a model](#train)\n",
    "- [3.4 Save as PMML file](#save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare data<a id=\"prep\"></a>\n",
    "\n",
    "In this subsection you will split your data into: \n",
    "- train data set\n",
    "- test data set\n",
    "- predict data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int((.8+.18)*len(df))])\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "predict_data = splitted_data[2]\n",
    "\n",
    "print(\"Number of training records: \" + str(len(train_data)))\n",
    "print(\"Number of testing records : \" + str(len(test_data)))\n",
    "print(\"Number of prediction records : \" + str(len(predict_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see your data has been successfully split into three data sets: \n",
    "\n",
    "-  The train data set, which is the largest group, is used for training.\n",
    "-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n",
    "-  The predict data set will be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create a ML model and pipeline<a id=\"pipe\"></a>\n",
    "\n",
    "In this section you will create a Scikit-Learn machine learning model and then train the model.\n",
    "\n",
    "In the first step you need to import the Scikit-Learn machine learning packages that will be needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the model. A linear model with Stochastic Gradient Descent is used in the following example. We use a pipeline to add an input scaling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=42, tol=1e-3)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then create a simple pipeline to first scale the input parameter values and then apply the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('standardize', scaler),\n",
    "    (\"classifier\", clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train the model<a id=\"train\"></a>\n",
    "Now, you can train your Random Forest model by using the previously defined **pipeline** and **train data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = train_data.loc[:, used_names[:-1]]\n",
    "y_train_data = train_data.loc[:, used_names[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train_data, y_train_data)\n",
    "\n",
    "# we defined a variable trainedAt to keep track of when the model was trained\n",
    "import datetime;\n",
    "ts = datetime.datetime.now()\n",
    "trainedAt = ts.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your **model accuracy** now. Use **test data** to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_data = test_data.loc[:, used_names[:-1]]\n",
    "y_test_data = test_data.loc[:, used_names[-1]]\n",
    "\n",
    "predictions = pipeline.predict(x_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a **metrics** variable to keep track of the metrics values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, classification_report, balanced_accuracy_score, accuracy_score, confusion_matrix\n",
    "\n",
    "metrics = []\n",
    "\n",
    "name = \"Coefficient of determination R^2\"\n",
    "r2 = pipeline.score(x_test_data, y_test_data)\n",
    "metrics.append({ \"name\": name, \"value\": r2 })\n",
    "\n",
    "name = \"Root Mean Squared Error (RMSE)\"\n",
    "rmse = mean_squared_error(y_test_data, predictions)\n",
    "metrics.append({ \"name\": name, \"value\": rmse })\n",
    "\n",
    "name = \"Accuracy\"\n",
    "acc = accuracy_score(y_test_data, predictions)\n",
    "metrics.append({ \"name\": name, \"value\": acc })\n",
    "\n",
    "name = \"Balanced accuracy\"\n",
    "balanced_acc = balanced_accuracy_score(y_test_data, predictions)\n",
    "metrics.append({ \"name\": name, \"value\": balanced_acc })\n",
    "\n",
    "name = \"Confusion Matrix\"\n",
    "confusion_mat = confusion_matrix(y_test_data, predictions, labels=[0, 1])\n",
    "metrics.append({ \"name\": name, \"value\": str(confusion_mat.tolist()) })\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric[\"name\"], \"on test data =\", metric[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_data, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Save as pmml file <a id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nyoka==4.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = type(clf).__name__\n",
    "scaler_name = type(scaler).__name__\n",
    "\n",
    "from nyoka import skl_to_pmml\n",
    "features=x_train_data.columns\n",
    "target=\"paymentDefault\"\n",
    "pmml_filename = \"ML-Sample-\" + model_name + '-' + scaler_name + \"-pmml.xml\"\n",
    "skl_to_pmml(pipeline, features, target, pmml_filename)\n",
    "print(pmml_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"provider\"></a>\n",
    "## 4. Store the model in Watson Machine Learning Provider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn how to use Python client libraries to store your pipeline and model in WML repository.\n",
    "\n",
    "- [4.1 Import the libraries](#lib)\n",
    "- [4.2 Save model](#save)\n",
    "- [4.3 Invoke model](#local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Import the libraries<a id=\"lib\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to the Watson Machine Learning service on IBM Cloud.\n",
    "\n",
    "**Tip**: Authentication information (your credentials) can be found in the <a href=\"https://cloud.ibm.com/iam/apikeys\" target=\"_blank\" rel=\"noopener no referrer\">Service credentials</a> tab of the service instance that you created on IBM Cloud. \n",
    "\n",
    "If you cannot see the **instance_id** field in **Service Credentials**, click **New credential (+)** to generate new authentication information. \n",
    "\n",
    "**Action**: Enter your Watson Machine Learning service instance credentials here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "wml_credentials = {\n",
    "                   \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "                   \"apikey\":\"PASTE YOUR API KEY HERE\"\n",
    "                  }\n",
    "\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Save the pipeline and deploy model<a id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to create a space that will be used for deploying models. If you do not have space already created, you can use  <a href=\"https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas\" target=\"_blank\" rel=\"noopener no referrer\">Deployment Spaces Dashboard</a> to create one.\n",
    "\n",
    "- Click New Deployment Space\n",
    "- Create an empty space\n",
    "- Select Cloud Object Storage\n",
    "- Select Watson Machine Learning instance and press Create\n",
    "- Copy space_id and paste it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_id = 'PASTE YOUR SPACE ID HERE'\n",
    "client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish model directly from pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sofware_spec_uid = client.software_specifications.get_id_by_name(\"spark-mllib_2.4\")\n",
    "\n",
    "metadata = {\n",
    "            client.repository.ModelMetaNames.NAME: 'Payment Default - PMML',\n",
    "            client.repository.ModelMetaNames.TYPE: 'pmml_4.3',\n",
    "            client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid,\n",
    "            client.repository.ModelMetaNames.LABEL_FIELD: 'paymentDefault',\n",
    "\n",
    "}\n",
    "\n",
    "published_model_details = client.repository.store_model(model=pmml_filename, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uid = client.repository.get_model_uid( published_model_details )\n",
    "\n",
    "print( \"model_uid: \", model_uid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name  = \"Payment Default deployement\"\n",
    "deployment_desc  = \"Online deployment of Loan payment default predictive service in pmml\"\n",
    "deployment_metadata = {\n",
    "                        client.deployments.ConfigurationMetaNames.NAME: deployment_name, \n",
    "                        client.deployments.ConfigurationMetaNames.DESCRIPTION: deployment_desc,\n",
    "                        client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "deployment       = client.deployments.create(artifact_uid=model_uid, meta_props=deployment_metadata)\n",
    "scoring_endpoint = client.deployments.get_scoring_href( deployment )\n",
    "print( \"scoring_endpoint: \", scoring_endpoint )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Use `client.repository.ModelMetaNames.show()` to get the list of available props."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.ModelMetaNames.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"local\"></a>\n",
    "### 4.3 Invoke model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will score the *predict_data* data set.\n",
    "You will learn how to invoke a saved model from a specified instance of Watson Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_id = client.deployments.get_id(deployment)\n",
    "\n",
    "x_predict_data = predict_data.loc[:, used_names[:-1]]\n",
    "y_predict_data = predict_data.loc[:, used_names[-1]]\n",
    "\n",
    "#scoring_payload = {\n",
    "#    \"fields\": x_predict_data.columns.values.tolist(),\n",
    "#    \"values\": x_predict_data.values.tolist()\n",
    "#}\n",
    "\n",
    "scoring_payload = {\n",
    "    client.deployments.ScoringMetaNames.INPUT_DATA: [\n",
    "        {\n",
    "            'fields': x_predict_data.columns.values.tolist(),\n",
    "            'values': x_predict_data.values.tolist()\n",
    "        }]\n",
    "}\n",
    "predictions_predict_data = client.deployments.score(deployment_id, scoring_payload)\n",
    "\n",
    "#print(json.dumps(predictions_predict_data, indent=4))\n",
    "predictions_predict_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview some results metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predictions = []\n",
    "for result in predictions_predict_data['predictions'][0].get('values'):\n",
    "    if result[0] >= 0.5:\n",
    "        label_predictions.append(0)\n",
    "    elif result[0] < 0.5:\n",
    "        label_predictions.append(1)\n",
    "        \n",
    "balanced_acc = balanced_accuracy_score(y_predict_data, label_predictions)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_predict_data, label_predictions, labels=[0, 1])\n",
    "\n",
    "acc = accuracy_score(y_predict_data, label_predictions)\n",
    "\n",
    "print('Accuracy', acc)\n",
    "print('Balanced accuracy', balanced_acc)\n",
    "print('Confusion Matrix', confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 5. Summary and next steps\n",
    "You successfully completed this notebook!   \n",
    "Check out our [Online Documentation](https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html) for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "This notebook was inspired by original notebook written by Pierre Feillet using Apache Spark and Watson Machine Learning.\n",
    "It was adapted for Scikit Learn by Marine Collery. A section to save the PMML was added by Amel Ben Othmane."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "458px",
    "left": "10px",
    "top": "150px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
