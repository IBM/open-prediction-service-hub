{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<table style=\"border: none\" align=\"left\">\n   <tr style=\"border: none\">\n      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Build a Loan default scoring model in Watson ML </b></th>\n      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n   </tr>\n</table>"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook is a simple example to build and deploy a Machine Learning model ready to be used in Automation Decision Service.\nThe model deployment is stored in Watson ML. \nThis other [notebook](https://github.com/icp4a/automation-decision-services-samples/tree/master/samples/MLNotebooks/Predict%20loan%20default%20with%20scikit-learn%20in%20WML.ipynb)\nbuilds the same model and stores it in OPS.\n\nNote that this model is built on a small dataset to serve as an example, its predictions are not realistic.\n\nAfter running this notebook, you can use the model deployment in a decision project in Automation Decision Service. You find a detail description for this kind of integration in the [ML Start tutorial](https://github.com/icp4a/automation-decision-services-samples/tree/master/samples/MLStart).\n\nSome familiarity with Python is helpful. This notebook uses Python 3.\n\n## Prerequisites\n\nTo use the notebooks, follow the documentation [Creating a project](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/projects.html?audience=wdp).\nOthers notebooks are available in this [Samples documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-samples-overview.html).\n\n## Learning goals\n\nYou will learn how to:\n\n-  Load a CSV file into a Pandas DataFrame.\n-  Explore data.\n-  Prepare data for training and evaluation.\n-  Create a scikit-learn machine learning model.\n-  Train and evaluate a model.\n\n\n## Contents\n\nThis notebook contains the following parts:\n\n1.\t[Set up](#setup)\n2.\t[Load and explore data](#load)\n3.\t[Create a Scikit learn machine learning model](#model)\n4.\t[Store the model in Watson Machine Learning provider](#provider)\n5.\t[Summary and next steps](#summary)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## 1. Set up\n\nBefore you use the sample code in this notebook, you create a <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a lite plan is offered and information about how to create the instance is <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load\"></a>\n## 2. Load and explore data"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will load the data as a Pandas DataFrame and perform a basic exploration.\n\nLoad the data to the Pandas DataFrame by using *wget* to upload the data to gpfs and then use pandas *read* method to read data. "}, {"metadata": {}, "cell_type": "code", "source": "# Install wget if you don't already have it.\n!pip install wget", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "import wget\nlink_to_data = 'https://raw.githubusercontent.com/ODMDev/decisions-on-spark/master/data/miniloan/miniloan-payment-default-cases-v2.0.csv'\nfilename = wget.download(link_to_data)\n\nprint(filename)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Import required libraires to create our Panda DataFrame"}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Load the file to Pandas DataFrame using code below"}, {"metadata": {}, "cell_type": "code", "source": "used_names = ['creditScore', 'income', 'loanAmount', 'monthDuration', 'rate', 'yearlyReimbursement', 'paymentDefault']\n\ndf = pd.read_csv(\n    filename,\n    header=0,\n    delimiter=r'\\s*,\\s*',\n    engine='python'\n).replace(\n    [np.inf, -np.inf], np.nan\n).dropna().loc[:, used_names]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Explore the loaded data by using the following Pandas DataFrame methods:\n-  print types\n-  print top ten records\n-  count all records"}, {"metadata": {}, "cell_type": "code", "source": "# convert all columns of DataFrame to float to avoid scaler warnings\ndf = df.astype({'creditScore': float, \"income\": np.float64, \"loanAmount\": np.float64, \"monthDuration\": np.float64, \"yearlyReimbursement\": np.float64, \"paymentDefault\": np.float64})\ndf.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see, the data contains five fields. default field is the one you would like to predict (label)."}, {"metadata": {}, "cell_type": "code", "source": "df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(\"Number of records: \" + str(len(df)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"model\"></a>\n## 3. Create a Scikit learn machine learning model\n\nIn this section you will learn how to:\n\n- [3.1 Prepare data](#prep)\n- [3.2 Create a model](#pipe)\n- [3.3 Train a model](#train)"}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1 Prepare data<a id=\"prep\"></a>\n\nIn this subsection you will split your data into: \n- train data set\n- test data set\n- predict data set"}, {"metadata": {}, "cell_type": "code", "source": "splitted_data = np.split(df.sample(frac=1, random_state=42), [int(.7*len(df)), int((.7+.2)*len(df))])\ntrain_data = splitted_data[0]\ntest_data = splitted_data[1]\npredict_data = splitted_data[2]\n\nprint(\"Number of training records: \" + str(len(train_data)))\nprint(\"Number of testing records : \" + str(len(test_data)))\nprint(\"Number of prediction records : \" + str(len(predict_data)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see your data has been successfully split into three data sets: \n\n-  The train data set, which is the largest group, is used for training.\n-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n-  The predict data set will be used for prediction."}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2 Create the model and pipeline<a id=\"pipe\"></a>\n\nIn this section you will create a Scikit-Learn machine learning model and then train the model.\n\nIn the first step you need to import the Scikit-Learn machine learning packages that will be needed in the subsequent steps."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now construct the model. A linear model with Stochastic Gradient Descent is used in the following example. We use a pipeline to add an input scaling step."}, {"metadata": {}, "cell_type": "code", "source": "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=42, tol=1e-3)\nscaler = StandardScaler()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You then create a simple pipeline to first scale the input parameter values and then apply the model."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('standardize', scaler),\n    (\"classifier\", clf)\n])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3 Train the model<a id=\"train\"></a>\nNow, you can train your Random Forest model by using the previously defined **pipeline** and **train data**."}, {"metadata": {}, "cell_type": "code", "source": "train_data.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "x_train_data = train_data.loc[:, used_names[:-1]]\ny_train_data = train_data.loc[:, used_names[-1]]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pipeline.fit(x_train_data, y_train_data)\n\n# we defined a variable trainedAt to keep track of when the model was trained\nimport datetime;\nts = datetime.datetime.now()\ntrainedAt = ts.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can check your **model accuracy** now. Use **test data** to evaluate the model."}, {"metadata": {}, "cell_type": "code", "source": "x_test_data = test_data.loc[:, used_names[:-1]]\ny_test_data = test_data.loc[:, used_names[-1]]\n\npredictions = pipeline.predict(x_test_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We define a **metrics** variable to keep track of the metrics values"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import mean_squared_error, classification_report, balanced_accuracy_score, accuracy_score, confusion_matrix\n\nmetrics = []\n\nname = \"Coefficient of determination R^2\"\nr2 = pipeline.score(x_test_data, y_test_data)\nmetrics.append({ \"name\": name, \"value\": r2 })\n\nname = \"Root Mean Squared Error (RMSE)\"\nrmse = mean_squared_error(y_test_data, predictions)\nmetrics.append({ \"name\": name, \"value\": rmse })\n\nname = \"Accuracy\"\nacc = accuracy_score(y_test_data, predictions)\nmetrics.append({ \"name\": name, \"value\": acc })\n\nname = \"Balanced accuracy\"\nbalanced_acc = balanced_accuracy_score(y_test_data, predictions)\nmetrics.append({ \"name\": name, \"value\": balanced_acc })\n\nname = \"Confusion Matrix\"\nconfusion_mat = confusion_matrix(y_test_data, predictions, labels=[0, 1])\nmetrics.append({ \"name\": name, \"value\": str(confusion_mat.tolist()) })\n\nfor metric in metrics:\n    print(metric[\"name\"], \"on test data =\", metric[\"value\"])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(classification_report(y_test_data, predictions))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"provider\"></a>\n## 4. Store the model in Watson Machine Learning provider\nIn this section you will learn how to use Python client libraries to store your pipeline and model in WML repository.\n- [4.1 Create a Watson Learning Machine client](#auth)\n- [4.2 Save model](#save)\n- [4.3 Invoke model](#local)\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1 Create a Watson Learning Machine client<a id=\"auth\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Authenticate to the Watson Machine Learning service on IBM Cloud.\n\n**Tip**: Authentication information (your credentials) can be found in the <a href=\"https://cloud.ibm.com/iam/apikeys\" target=\"_blank\" rel=\"noopener no referrer\">Service credentials</a> tab of the service instance that you created on IBM Cloud. \n\n**Action**: Enter your Watson Machine Learning service instance credentials in the following cell, then change its type to code."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nwml_credentials = {\n                   \"url\": \"URL TO BE SET\",\n                   \"apikey\":\"API KEY TO BE SET\"\n                  }\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.2 Save the pipeline and deploy model<a id=\"save\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "In this subsection you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."}, {"metadata": {}, "cell_type": "markdown", "source": "First, you need to create a space that will be used for deploying models. If you do not have space already created, you can use  <a href=\"https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas\" target=\"_blank\" rel=\"noopener no referrer\">Deployment Spaces Dashboard</a> to create one.\n\n- Click New Deployment Space\n- Create an empty space\n- Select Cloud Object Storage\n- Select Watson Machine Learning instance and press Create\n- Copy space_id and paste it in the cell below\n- Change the next cell type as code."}, {"metadata": {}, "cell_type": "code", "source": "space_id = 'SPACE ID TO BE SET'\nclient.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Publish model directly from pipeline."}, {"metadata": {}, "cell_type": "code", "source": "input_data_schema={\n    'id': '1', \n    'type': 'struct', \n    'fields': [\n        {  \n            'name': 'creditScore',\n            'nullable': True,\n            'type': 'float64'\n        },\n        {   \n            'name': 'income',\n            'nullable': True,\n            'type': 'float64'\n        },\n        {   \n            'name': 'loanAmount',\n            'nullable': True,\n            'type': 'float64'\n        },\n        {   \n            'name': 'monthDuration',\n            'nullable': True,\n            'type': 'float64'\n        },\n        {  \n            'name': 'rate',\n            'nullable': True,\n            'type': 'float64'\n        },\n        {   \n            'name': 'yearlyReimbursement',\n            'nullable': True,\n            'type': 'float64'\n        }\n]}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Please update if necessary the software configuration given by Watson Studio kernel\nsofware_spec_uid = client.software_specifications.get_id_by_name(\"scikit-learn_0.22-py3.6\")\n\nmetadata = {\n     client.repository.ModelMetaNames.NAME: 'Loan Risk Score scikit-learn SGDClassifier',\n     client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.22',\n     client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid,\n     client.repository.ModelMetaNames.LABEL_FIELD: 'paymentDefault',\n     client.repository.ModelMetaNames.INPUT_DATA_SCHEMA: input_data_schema\n\n }\n\npublished_model_details = client.repository.store_model(model=pipeline, meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_uid = client.repository.get_model_uid( published_model_details )\n\nprint( \"model_uid: \", model_uid )", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "deployment_name  = \"loan risk score\"\ndeployment_desc  = \"Online deployment of Loan payment default predictive service\"\ndeployment_metadata = {\n                        client.deployments.ConfigurationMetaNames.NAME: deployment_name, \n                        client.deployments.ConfigurationMetaNames.DESCRIPTION: deployment_desc,\n                        client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\ndeployment       = client.deployments.create(artifact_uid=model_uid, meta_props=deployment_metadata)\nscoring_endpoint = client.deployments.get_scoring_href( deployment )\nprint( \"scoring_endpoint: \", scoring_endpoint )", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**: Use `client.repository.ModelMetaNames.show()` to get the list of available props."}, {"metadata": {}, "cell_type": "code", "source": "client.repository.ModelMetaNames.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"local\"></a>\n### 4.3 Invoke model\n"}, {"metadata": {}, "cell_type": "markdown", "source": "In this subsection you will score the *predict_data* data set.\nYou will learn how to invoke a saved model from a specified instance of Watson Machine Learning."}, {"metadata": {}, "cell_type": "markdown", "source": "Here is an example of the expected output:\n\n{'predictions': [{'fields': ['prediction', 'probability'],\n   'values': [[0, [0.9999999999624466, 3.75534112651071e-11]],\n    [0, [0.9999999899354463, 1.006455371108775e-08]],\n    [1, [8.500722443516295e-06, 0.9999914992775565]],\n    ...,\n    [0, [0.9999999999999991, 9.024583530101307e-16]]]}]}"}, {"metadata": {}, "cell_type": "code", "source": "deployment_id = client.deployments.get_id(deployment)\n\nx_predict_data = predict_data.loc[:, used_names[:-1]]\ny_predict_data = predict_data.loc[:, used_names[-1]]\n\nscoring_payload = {\n    client.deployments.ScoringMetaNames.INPUT_DATA: [\n        {\n            'fields': x_predict_data.columns.values.tolist(),\n            'values': x_predict_data.values.tolist()\n        }]\n}\npredictions_predict_data = client.deployments.score(deployment_id, scoring_payload)\n\npredictions_predict_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Preview some results metrics"}, {"metadata": {}, "cell_type": "code", "source": "label_predictions = []\nfor result in predictions_predict_data['predictions'][0].get('values'):\n    if result[0] >= 0.5:\n        label_predictions.append(0)\n    elif result[0] < 0.5:\n        label_predictions.append(1)\n        \nbalanced_acc = balanced_accuracy_score(y_predict_data, label_predictions)\n\nconfusion_mat = confusion_matrix(y_predict_data, label_predictions, labels=[0, 1])\n\nacc = accuracy_score(y_predict_data, label_predictions)\n\nprint('Accuracy', acc)\nprint('Balanced accuracy', balanced_acc)\nprint('Confusion Matrix', confusion_mat)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## 5. Summary and next steps\nYou successfully completed this notebook! \n \nYou learned how to use Scikit Learn machine learning API for model creation and deployment. \n \nNow you can use this model deployment in a decision project in Automation Decision Service. You find a detail description for this kind of integration in the  [ML Start tutorial](https://github.com/icp4a/automation-decision-services-samples/tree/master/samples/MLStart). "}, {"metadata": {}, "cell_type": "markdown", "source": "### Authors\n\nThis notebook was inspired by original notebook written by Pierre Feillet using Apache Spark and Watson Machine Learning.\nIt was adapted for Scikit Learn by Marine Collery. It was updated to use the latest version of WML by Amel Ben Othmane. "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {"height": "458px", "left": "10px", "top": "150px", "width": "212px"}, "toc_section_display": true, "toc_window_display": true}}, "nbformat": 4, "nbformat_minor": 1}