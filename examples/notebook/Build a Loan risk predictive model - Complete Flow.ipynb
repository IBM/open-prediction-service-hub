{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Build a Loan default scoring model and service</b></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains steps and code to get a loan dataset, create a predictive model, and start scoring new data. This notebook introduces commands for getting data and for basic data cleaning and exploration, model creation, model training, model persistence to the Open Prediction Service, model deployment, and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "-  Load a CSV file into a Pandas DataFrame.\n",
    "-  Explore data.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create a scikit-learn machine learning model.\n",
    "-  Train and evaluate a model.\n",
    "-  Store and deploy a model in Open Predictive Service.\n",
    "-  Score sample scoring data using a Open Predictive Service invocation.\n",
    "-  Explore and visualize the prediction result using the plotly package.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Set up](#setup)\n",
    "2.\t[Load and explore data](#load)\n",
    "3.\t[Create a Scikit learn machine learning model](#model)\n",
    "4.\t[Store the model in the provider of your choice](#provider)\n",
    "5.\t[Use Plotly to visualize data](#plotly)\n",
    "6.\t[Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Set up\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "You will need at least one provider to use your model in ADS.\n",
    "\n",
    "For a **Watson Machine Learning provider** you must:\n",
    "- Create a <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a lite plan is offered and information about how to create the instance is <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)\n",
    "\n",
    "For an **Open Prediciton Service provider** you must:\n",
    "\n",
    "**TODO change LINK to documentation LINK**\n",
    "-  Create a <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\" rel=\"noopener no referrer\">Open Prediction Service</a> instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 2. Load and explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will load the data as a Pandas DataFrame and perform a basic exploration.\n",
    "\n",
    "Load the data to the Pandas DataFrame by using *wget* to upload the data to gpfs and then use pandas *read* method to read data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wget if you don't already have it.\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "#link_to_data = 'https://raw.githubusercontent.com/ODMDev/decisions-on-spark/master/data/miniloan/miniloan-riskscore-1K-v1.0.csv'\n",
    "#link_to_data = 'https://raw.githubusercontent.com/ODMDev/decisions-on-spark/master/data/miniloan/miniloan-payment-default-risk-v2.0.csv'\n",
    "link_to_data = 'https://raw.githubusercontent.com/ODMDev/decisions-on-spark/master/data/miniloan/miniloan-payment-default-cases-v2.0.csv'\n",
    "\n",
    "filename = wget.download(link_to_data)\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraires to create our Panda DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file to Pandas DataFrame using code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_names = ['creditScore', 'income', 'loanAmount', 'monthDuration', 'rate', 'yearlyReimbursement', 'paymentDefault']\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filename,\n",
    "    header=0,\n",
    "    delimiter=r'\\s*,\\s*',\n",
    "    engine='python'\n",
    ").replace(\n",
    "    [np.inf, -np.inf], np.nan\n",
    ").dropna().loc[:, used_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the loaded data by using the following Pandas DataFrame methods:\n",
    "-  print types\n",
    "-  print top ten records\n",
    "-  count all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data contains five fields. default field is the one you would like to predict (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records: \" + str(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 3. Create a Scikit learn machine learning model\n",
    "\n",
    "In this section you will learn how to:\n",
    "\n",
    "- [3.1 Prepare data](#prep)\n",
    "- [3.2 Create a model](#pipe)\n",
    "- [3.3 Train a model](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare data<a id=\"prep\"></a>\n",
    "\n",
    "In this subsection you will split your data into: \n",
    "- train data set\n",
    "- test data set\n",
    "- predict data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int((.8+.18)*len(df))])\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "predict_data = splitted_data[2]\n",
    "\n",
    "print(\"Number of training records: \" + str(len(train_data)))\n",
    "print(\"Number of testing records : \" + str(len(test_data)))\n",
    "print(\"Number of prediction records : \" + str(len(predict_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see your data has been successfully split into three data sets: \n",
    "\n",
    "-  The train data set, which is the largest group, is used for training.\n",
    "-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n",
    "-  The predict data set will be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create a model<a id=\"pipe\"></a>\n",
    "\n",
    "In this section you will create a Scikit-Learn machine learning model and then train the model.\n",
    "\n",
    "In the first step you need to import the Scikit-Learn machine learning packages that will be needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the model. A linear model with Stochastic Gradient Descent is used in the following example. We use a pipeline to add an input scaling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=42, tol=1e-3)\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('standardize', scaler),\n",
    "    (\"classifier\", clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train the model<a id=\"train\"></a>\n",
    "Now, you can train your Random Forest model by using the previously defined **pipeline** and **train data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = train_data.loc[:, used_names[:-1]]\n",
    "y_train_data = train_data.loc[:, used_names[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train_data, y_train_data)\n",
    "\n",
    "# we defined a variable trainedAt to keep track of when the model was trained\n",
    "import datetime;\n",
    "ts = datetime.datetime.now()\n",
    "trainedAt = ts.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your **model accuracy** now. Use **test data** to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_data = test_data.loc[:, used_names[:-1]]\n",
    "y_test_data = test_data.loc[:, used_names[-1]]\n",
    "\n",
    "predictions = pipeline.predict(x_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a **metrics** variable to keep track of the metrics values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, classification_report, balanced_accuracy_score, accuracy_score, confusion_matrix\n",
    "\n",
    "metrics = []\n",
    "\n",
    "name = \"Coefficient of determination R^2\"\n",
    "r2 = pipeline.score(x_test_data, y_test_data)\n",
    "metrics.append({ \"name\": name, \"value\": r2 })\n",
    "\n",
    "name = \"Root Mean Squared Error (RMSE)\"\n",
    "rmse = mean_squared_error(y_test_data, predictions)\n",
    "metrics.append({ \"name\": name, \"value\": rmse })\n",
    "\n",
    "name = \"Accuracy\"\n",
    "acc = accuracy_score(y_test_data, predictions)\n",
    "metrics.append({ \"name\": name, \"value\": acc })\n",
    "\n",
    "name = \"Balanced accuracy\"\n",
    "balanced_acc = balanced_accuracy_score(y_test_data, predictions)\n",
    "metrics.append({ \"name\": name, \"value\": balanced_acc })\n",
    "\n",
    "name = \"Confusion Matrix\"\n",
    "confusion_mat = confusion_matrix(y_test_data, predictions, labels=[0, 1])\n",
    "metrics.append({ \"name\": name, \"value\": str(confusion_mat.tolist()) })\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric[\"name\"], \"on test data =\", metric[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_data, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Save as pmml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn2pmml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = type(clf).__name__\n",
    "scaler_name = type(scaler).__name__\n",
    "\n",
    "from sklearn2pmml import make_pmml_pipeline, sklearn2pmml\n",
    "\n",
    "pmml_pipeline = make_pmml_pipeline(\n",
    "    pipeline,\n",
    "    active_fields=x_train_data.columns,\n",
    "    target_fields=['paymentDefault']\n",
    ")\n",
    "pmml_filename = \"ML-Sample-\" + model_name + '-' + scaler_name + \"-pmml.xml\"\n",
    "sklearn2pmml(pmml_pipeline, pmml_filename, with_repr = True)\n",
    "print(pmml_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"provider\"></a>\n",
    "## 4. Store the model in the provider of your choice\n",
    "In this section you will learn how to use Python client libraries to store your model in the provider of your choice.\n",
    "\n",
    "**Action** Click the provider you want to use.\n",
    "1.\t[Watson Machine Learning provider](#wml)\n",
    "2.\t[Open Prediction Service provider](#ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"wml\"></a>\n",
    "### 4.1 Watson Machine Learning provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn how to use Python client libraries to store your pipeline and model in WML repository.\n",
    "\n",
    "- [4.1.1 Import the libraries](#lib)\n",
    "- [4.1.2 Save model](#save)\n",
    "- [4.1.3 Invoke model](#local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Import the libraries<a id=\"lib\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, you must install and import the `watson-machine-learning-client` libraries.\n",
    "\n",
    "**Note**: Python 3.5 and Apache Spark 2.1 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $PIP_BUILD/watson-machine-learning-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install watson-machine-learning-client --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to the Watson Machine Learning service on IBM Cloud.\n",
    "\n",
    "**Tip**: Authentication information (your credentials) can be found in the <a href=\"https://console.bluemix.net/docs/services/service_credentials.html#service_credentials\" target=\"_blank\" rel=\"noopener no referrer\">Service credentials</a> tab of the service instance that you created on IBM Cloud. \n",
    "\n",
    "If you cannot see the **instance_id** field in **Service Credentials**, click **New credential (+)** to generate new authentication information. \n",
    "\n",
    "**Action**: Enter your Watson Machine Learning service instance credentials here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "wml_credentials = {\n",
    "  \"apikey\": \"TO REPLACE\",\n",
    "  \"iam_apikey_description\": \"TO REPLACE\",\n",
    "  \"iam_apikey_name\": \"TO REPLACE\",\n",
    "  \"iam_role_crn\": \"TO REPLACE\",\n",
    "  \"iam_serviceid_crn\": \"TO REPLACE\",\n",
    "  \"instance_id\": \"421f8abf-1cc1-4aa8-80a0-0fb491f48308\",\n",
    "  \"url\": \"TO REPLACE\"\n",
    "}\n",
    "\n",
    "client = WatsonMachineLearningAPIClient( wml_credentials )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Save the pipeline and deploy model<a id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish model based on PMML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    client.repository.ModelMetaNames.NAME: 'Loan Fraud Detection - PMML',\n",
    "    client.repository.ModelMetaNames.FRAMEWORK_NAME: 'pmml',\n",
    "    client.repository.ModelMetaNames.FRAMEWORK_VERSION: '4.4',\n",
    "    client.repository.ModelMetaNames.RUNTIME_NAME: 'java',\n",
    "    client.repository.ModelMetaNames.RUNTIME_VERSION: '1.8',\n",
    "    client.repository.ModelMetaNames.EVALUATION_METHOD: 'multiclass',\n",
    "    client.repository.ModelMetaNames.EVALUATION_METRICS: metrics\n",
    "}\n",
    "\n",
    "published_model_details = client.repository.store_model(pmml_filename, meta_props=metadata, training_data=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish model directly from pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata = {\n",
    "#     client.repository.ModelMetaNames.NAME: 'Loan Fraud Detection - Scikit Learn',\n",
    "#     client.repository.ModelMetaNames.FRAMEWORK_NAME: 'scikit-learn',\n",
    "#     client.repository.ModelMetaNames.FRAMEWORK_VERSION: '0.20',\n",
    "#     client.repository.ModelMetaNames.RUNTIME_NAME: 'python',\n",
    "#     client.repository.ModelMetaNames.RUNTIME_VERSION: '3.6',\n",
    "#     client.repository.ModelMetaNames.LABEL_FIELD: 'paymentDefault'\n",
    "#     client.repository.ModelMetaNames.EVALUATION_METRICS: metrics\n",
    "# }\n",
    "\n",
    "# published_model_details = client.repository.store_model(model=pipeline, meta_props=metadata, training_data=x_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uid = client.repository.get_model_uid( published_model_details )\n",
    "\n",
    "print( \"model_uid: \", model_uid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploymnt_name   = \"fraud prediction\"\n",
    "deployment_desc  = \"Online deployment of Loan payment default predictive service\"\n",
    "deployment       = client.deployments.create( model_uid, deploymnt_name, deployment_desc )\n",
    "scoring_endpoint = client.deployments.get_scoring_url( deployment )\n",
    "print( \"scoring_endpoint: \", scoring_endpoint )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Use `client.repository.ModelMetaNames.show()` to get the list of available props."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.ModelMetaNames.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"local\"></a>\n",
    "#### 4.1.3 Invoke model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will score the *predict_data* data set.\n",
    "You will learn how to invoke a saved model from a specified instance of Watson Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict_data = predict_data.loc[:, used_names[:-1]]\n",
    "y_predict_data = predict_data.loc[:, used_names[-1]]\n",
    "\n",
    "scoring_payload = {\n",
    "    \"fields\": x_predict_data.columns.values.tolist(),\n",
    "    \"values\": x_predict_data.values.tolist()\n",
    "}\n",
    "predictions_predict_data = client.deployments.score(scoring_endpoint, scoring_payload)\n",
    "\n",
    "print(json.dumps(predictions_predict_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview some results metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predictions = []\n",
    "for result in predictions_predict_data['values']:\n",
    "    if result[0] >= 0.5:\n",
    "        label_predictions.append(0)\n",
    "    elif result[0] < 0.5:\n",
    "        label_predictions.append(1)\n",
    "        \n",
    "balanced_acc = balanced_accuracy_score(y_predict_data, label_predictions)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_predict_data, label_predictions, labels=[0, 1])\n",
    "\n",
    "acc = accuracy_score(y_predict_data, label_predictions)\n",
    "\n",
    "print('Accuracy', acc)\n",
    "print('Balanced accuracy', balanced_acc)\n",
    "print('Confusion Matrix', confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provider is all setup and ready to be used in ADS ?\n",
    "You can now go to section \n",
    "[WIP Use Plotly to visualize data](#visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ops\"></a>\n",
    "### 4.2 Open Prediction Service provider\n",
    "In this section you will learn how to use Python client libraries to store your model in your Open Predicitve Service.\n",
    "\n",
    "- [4.2.1 Set up](#lib)\n",
    "- [4.2.2 Deploy model](#save)\n",
    "- [4.2.3 Invoke the model](#load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Set up <a id=\"lib\"></a>\n",
    "\n",
    "In order to save your model into your Open Prediciton Service\n",
    "You must first:\n",
    "\n",
    "- Check that your Open Prediciton Service is up and running\n",
    "- Define a model configuration\n",
    "- Save your model in a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that your Open Prediciton Service is up and running\n",
    "\n",
    "**Action**: Enter your Open Prediciton Service instance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPS_REQUEST_URL = 'http://localhost:8080/v1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin, urlparse\n",
    "import json, requests\n",
    "\n",
    "# Checking that that Open Prediciton Service is up and running\n",
    "parsedUrl = urlparse(OPS_REQUEST_URL)\n",
    "statusUrl = urljoin(OPS_REQUEST_URL, urljoin(parsedUrl.path, 'status'))\n",
    "r = requests.get(statusUrl)\n",
    "\n",
    "status = r.status_code == requests.codes.ok\n",
    "\n",
    "if status:\n",
    "    print('Open Prediciton Service is up and running')\n",
    "    print(json.loads(r.text)[u'model_count'], 'models are alreday deployed')\n",
    "else:\n",
    "    print('An error occured when reaching out to your Open Prediciton Service instance', r.status_code, r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to define a configuration for your model.\n",
    "**TODO LINK TO DOC ?**\n",
    "\n",
    "**Action**: Complete all required data in the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters defining the model unicity:\n",
    "MODEL_NAME = \"loan-risk\"\n",
    "MODEL_VERSION = \"v0\"\n",
    "\n",
    "# Complementary parameters\n",
    "METHOD_NAME = \"predict_proba\"\n",
    "# For classification problems\n",
    "CLASS_NAMES = {\n",
    "    \"0\": \"False\",\n",
    "    \"1\": \"True\"\n",
    "}\n",
    "\n",
    "# Metadata\n",
    "METADATA_DESCRIPTION = \"Sample loan risk predictive model\"\n",
    "METADATA_AUTHOR = \"ADD_YOUR_NAME\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are automating the input and output schema generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import build_table_schema\n",
    "\n",
    "mappingToOPSSchema = {\n",
    "    'integer': 'int64',\n",
    "    'number': 'float64'\n",
    "}\n",
    "\n",
    "def getInputSchema(dataFrame):\n",
    "    inputSchema = build_table_schema(dataFrame, index=False, version=False)\n",
    "\n",
    "    for index, field in enumerate(inputSchema['fields']):\n",
    "        inputSchema['fields'][index]['type'] = mappingToOPSSchema[field['type']]\n",
    "        inputSchema['fields'][index]['order'] = index\n",
    "    return inputSchema['fields']\n",
    "\n",
    "# attributes schema for regression models for example\n",
    "# (probabilites)\n",
    "predictionAsFloatAttributesSchema = [\n",
    "    {\n",
    "        \"name\": \"prediction\",\n",
    "        \"type\": \"float\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# attributes schema for classification models\n",
    "# (label and probabilites)\n",
    "predictionAsStringOutputSchema = [\n",
    "    {\n",
    "        \"name\": \"prediction\",\n",
    "        \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"probabilities\",\n",
    "        \"type\": \"[Probability]\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving input and output schema\n",
    "inputSchema = getInputSchema(x_train_data)\n",
    "\n",
    "outputSchema = {\n",
    "    \"attributes\": predictionAsFloatAttributesSchema\n",
    "}\n",
    "\n",
    "if METHOD_NAME == 'predict_proba':\n",
    "    outputSchema['attributes'] = predictionAsStringOutputSchema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have a complete configuration object to be bundled with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configuration = {\n",
    "  \"name\": MODEL_NAME,\n",
    "  \"version\": MODEL_VERSION,\n",
    "  \"method_name\": METHOD_NAME,\n",
    "  \"input_schema\": inputSchema,\n",
    "  \"output_schema\": outputSchema,\n",
    "  \"metadata\": {\n",
    "    \"class_names\": CLASS_NAMES,\n",
    "    \"description\": METADATA_DESCRIPTION,\n",
    "    \"author\": METADATA_AUTHOR,\n",
    "    \"trained_at\": trainedAt,\n",
    "    \"metrics\": metrics\n",
    "  }\n",
    "}\n",
    "\n",
    "print(json.dumps(model_configuration, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your model in a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_pickle(pickle_filename, model, model_configuration):\n",
    "        with open(pickle_filename, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'model': model,\n",
    "                'model_config': model_configuration\n",
    "            }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_filename = MODEL_NAME + '-' + MODEL_VERSION + '-archive.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_pickle(pickle_filename, pipeline, model_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Deploy model<a id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUrl = urljoin(OPS_REQUEST_URL, urljoin(parsedUrl.path, 'models'))\n",
    "print(modelUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {'file': open(pickle_filename, 'rb')}\n",
    "\n",
    "r = requests.post(modelUrl, files=files)\n",
    "\n",
    "status = r.status_code == requests.codes.ok\n",
    "\n",
    "if status:\n",
    "    print('Model was succesfully deployed.')\n",
    "else:\n",
    "    print('Model was not deployed:', r.status_code, r.text)\n",
    "    print('You might want to check if your model does not alreday exist under the same name and version.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Invoke the model<a id=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invokeUrl = urljoin(OPS_REQUEST_URL, urljoin(parsedUrl.path, 'invocations'))\n",
    "print(invokeUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict_data = predict_data.loc[:, used_names[:-1]]\n",
    "y_predict_data = predict_data.loc[:, used_names[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predict_data = x_predict_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "data = {\n",
    "  \"model_name\": MODEL_NAME,\n",
    "  \"model_version\": MODEL_VERSION,\n",
    "  \"params\": []\n",
    "}\n",
    "featureLabels = x_predict_data.columns;\n",
    "\n",
    "predictions_np = []\n",
    "\n",
    "for row in raw_predict_data:\n",
    "    tmpData = deepcopy(data)\n",
    "    for index, value in enumerate(row):\n",
    "        tmpData['params'].append({\n",
    "            \"name\": featureLabels[index],\n",
    "            \"value\": value\n",
    "        })\n",
    "    print(json.dumps(tmpData, indent=4))\n",
    "    r = requests.post(invokeUrl,  data=json.dumps(tmpData))\n",
    "    status = r.status_code == requests.codes.ok\n",
    "    if status:\n",
    "        result = json.loads(r.text)[u'prediction']\n",
    "        predictions_np.append(result)\n",
    "    else:\n",
    "        print('Model was not invoked:', r.status_code, r.text)\n",
    "        break\n",
    "\n",
    "predictions_np = np.array(predictions_np, dtype=object)\n",
    "predictions_np = predictions_np == \"True\"\n",
    "\n",
    "predictions = pd.DataFrame(data=predictions_np, columns=[\"prediction\"])\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_data_Bool = y_predict_data.astype({\"paymentDefault\": bool})\n",
    "\n",
    "print(y_predict_data_Bool.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_acc = balanced_accuracy_score(y_predict_data_Bool, predictions)\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_predict_data_Bool, predictions, labels=[0, 1])\n",
    "\n",
    "acc = accuracy_score(y_predict_data_Bool, predictions)\n",
    "\n",
    "print('Accuracy', acc)\n",
    "print('Balanced accuracy', balanced_acc)\n",
    "print('Confusion Matrix', confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plotly\"></a>\n",
    "## 5. Use Plotly to visualize data\n",
    "\n",
    "In this subsection you will use the Plotly package to explore the prediction results. Plotly is an online analytics and data visualization tool.\n",
    "\n",
    "First, you need to install the required packages. You can do it by running the following code. Run it one time only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"notebook>=5.3\" \"ipywidgets>=7.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Plotly and the other required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data.index.equals(predictions.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not predict_data.index.equals(predictions.index):\n",
    "    predict_data = predict_data.reset_index()\n",
    "    predictions = pd.concat([predictions, predict_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_stats = predictions.groupby(['prediction']).count()\n",
    "product_data = [go.Pie(labels=cumulative_stats.index, values=cumulative_stats['income'])]\n",
    "product_layout = go.Layout(title='Predicted default income distribution')\n",
    "\n",
    "fig = go.Figure(data=product_data, layout=product_layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data set, you might want to do some analysis of the mean loan amount by using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_data = [go.Bar(y=predictions.groupby(['prediction']).mean()[\"loanAmount\"], x=cumulative_stats.index)]\n",
    "\n",
    "age_layout = go.Layout(\n",
    "    title='Mean loanAmount per predicted default',\n",
    "    xaxis=dict(title = \"default\", showline=False),\n",
    "    yaxis=dict(title = \"loanAmount\"))\n",
    "\n",
    "fig = go.Figure(data=age_data, layout=age_layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the bar plot you created, you might make the following conclusion: The mean amount for loan that present a default are 100k higher than the loans for which there is no default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 6. Summary and next steps\n",
    "You successfully completed this notebook! \n",
    " \n",
    "You learned how to use Scikit Learn machine learning API as well as Open Prediction Service for model creation and deployment. \n",
    " \n",
    "Check out our [Online Documentation](https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html) for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "This notebook was inspired by original notebook written by Pierre Feillet using Apache Spark and Watson Machine Learning.\n",
    "It was adapted for Scikit Learn and Open Prediction Service by Marine Collery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "458px",
    "left": "10px",
    "top": "150px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
